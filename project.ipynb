{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "103f2188-0808-4ef0-b025-d7cf279ab08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import os\n",
    "import pyautogui\n",
    "import joblib\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "005528b7-4ed6-4a31-a8d4-77e6205bf50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)  # Use webcam\n",
    "\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(static_image_mode=False, max_num_hands=1, min_detection_confidence=0.7)\n",
    "mp_draw = mp.solutions.drawing_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d977a5ae-1ea6-4c90-b6fa-df5927d4330f",
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    success, img = cap.read()\n",
    "    if not success:\n",
    "        continue\n",
    "    \n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    results = hands.process(img_rgb)\n",
    "    \n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            mp_draw.draw_landmarks(img, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "    \n",
    "    cv2.imshow(\"Hand Tracking\", img)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8809b0f-549f-476d-b51c-a574f32679c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize MediaPipe Hands\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(static_image_mode=False, max_num_hands=1, min_detection_confidence=0.7)\n",
    "mp_drawing = mp.solutions.drawing_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f7da09-8663-4c21-b168-49fdaab43642",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset directory\n",
    "DATA_DIR = 'gesture_data'\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    os.makedirs(DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4fd7a8-7a82-4ad2-9ccb-6879f8669656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your gestures\n",
    "gestures = ['index_up', 'fist', 'palm_open']\n",
    "samples_per_gesture = 100  # Number of samples to collect per gesture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22e654ff-13d3-46c8-8fbb-99d4548572b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting data for index_up. Press \"q\" to skip this gesture.\n",
      "Get ready in 3 seconds...\n",
      "Collecting data for fist. Press \"q\" to skip this gesture.\n",
      "Get ready in 3 seconds...\n",
      "Collecting data for palm_open. Press \"q\" to skip this gesture.\n",
      "Get ready in 3 seconds...\n"
     ]
    }
   ],
   "source": [
    "# Initialize webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "for gesture in gestures:\n",
    "    # Create subdirectory for each gesture\n",
    "    gesture_dir = os.path.join(DATA_DIR, gesture)\n",
    "    if not os.path.exists(gesture_dir):\n",
    "        os.makedirs(gesture_dir)\n",
    "    \n",
    "    print(f'Collecting data for {gesture}. Press \"q\" to skip this gesture.')\n",
    "    print('Get ready in 3 seconds...')\n",
    "    sleep(3)\n",
    "    \n",
    "    sample_count = 0\n",
    "    while sample_count < samples_per_gesture:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            continue\n",
    "            \n",
    "        # Flip frame horizontally for a mirror effect\n",
    "        frame = cv2.flip(frame, 1)\n",
    "        \n",
    "        # Convert to RGB and process with MediaPipe\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results = hands.process(frame_rgb)\n",
    "        \n",
    "        # Display countdown on screen\n",
    "        cv2.putText(frame, f'Collecting {gesture}: Sample {sample_count+1}/{samples_per_gesture}', \n",
    "                   (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "        \n",
    "        if results.multi_hand_landmarks:\n",
    "            hand_landmarks = results.multi_hand_landmarks[0]\n",
    "            \n",
    "            # Draw hand landmarks\n",
    "            mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "            \n",
    "            # Extract landmarks and save\n",
    "            landmarks = []\n",
    "            for landmark in hand_landmarks.landmark:\n",
    "                landmarks.extend([landmark.x, landmark.y, landmark.z])\n",
    "            \n",
    "            np.save(os.path.join(gesture_dir, f'{sample_count}.npy'), np.array(landmarks))\n",
    "            sample_count += 1\n",
    "            \n",
    "            # Small delay between samples\n",
    "            sleep(0.1)\n",
    "        \n",
    "        cv2.imshow('Data Collection', frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20cf7686-f6a8-4769-8682-d90002e39ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b929238-4f3b-4359-b84d-981e2f263966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "gestures = [\"index_up\", \"fist\", \"palm_open\"]\n",
    "X, y = [], []\n",
    "\n",
    "for idx, gesture in enumerate(gestures):\n",
    "    for file in os.listdir(f\"gesture_data/{gesture}\"):\n",
    "        data = np.load(f\"gesture_data/{gesture}/{file}\")\n",
    "        X.append(data)\n",
    "        y.append(idx)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "model = SVC(kernel='linear')\n",
    "model.fit(X_train, y_train)\n",
    "print(\"Accuracy:\", model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d8fde8e-d7bd-4610-b38a-8046d47b1869",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gesture_model.pkl']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "joblib.dump(model, \"gesture_model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "31f9623d-9261-4632-a16f-a16737b58148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize MediaPipe Hands\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(\n",
    "    static_image_mode=False,\n",
    "    max_num_hands=1,\n",
    "    min_detection_confidence=0.7,\n",
    "    min_tracking_confidence=0.5\n",
    ")\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Initialize video capture\n",
    "cap = cv2.VideoCapture(0)  # 0 for default camera\n",
    "\n",
    "# Load the trained gesture recognition model\n",
    "try:\n",
    "    model = joblib.load(\"gesture_model.pkl\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: Model file 'gesture_model.pkl' not found.\")\n",
    "    exit(1)\n",
    "\n",
    "# Get screen dimensions\n",
    "screen_width, screen_height = pyautogui.size()\n",
    "\n",
    "# Set pyautogui failsafe (move mouse to corner to abort)\n",
    "pyautogui.FAILSAFE = True\n",
    "\n",
    "# For smoothing cursor movement\n",
    "prev_x, prev_y = 0, 0\n",
    "smoothing_factor = 0.5\n",
    "\n",
    "while True:\n",
    "    success, img = cap.read()\n",
    "    if not success:\n",
    "        continue\n",
    "    \n",
    "    # Flip image horizontally for mirror effect\n",
    "    img = cv2.flip(img, 1)\n",
    "    \n",
    "    # Convert to RGB for MediaPipe\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Process hand landmarks\n",
    "    results = hands.process(img_rgb)\n",
    "    \n",
    "    if results.multi_hand_landmarks:\n",
    "        hand_landmarks = results.multi_hand_landmarks[0]\n",
    "        \n",
    "        # Draw hand landmarks on image\n",
    "        mp_drawing.draw_landmarks(\n",
    "            img, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "        \n",
    "        # Extract landmarks and normalize\n",
    "        data = []\n",
    "        for landmark in hand_landmarks.landmark:\n",
    "            data.extend([landmark.x, landmark.y, landmark.z])\n",
    "        \n",
    "        # Make prediction\n",
    "        try:\n",
    "            prediction = model.predict([data])[0]\n",
    "            \n",
    "            # Perform actions based on prediction\n",
    "            if prediction == 0:  # ðŸ‘† Index Up â†’ Left Click\n",
    "                pyautogui.click()\n",
    "                cv2.putText(img, \"Left Click\", (10, 50), \n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "                \n",
    "            elif prediction == 1:  # âœŠ Fist â†’ Right Click\n",
    "                pyautogui.rightClick()\n",
    "                cv2.putText(img, \"Right Click\", (10, 50), \n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "                \n",
    "            elif prediction == 2:  # âœ‹ Palm Open â†’ Move Cursor\n",
    "                # Get index finger tip coordinates (landmark 8)\n",
    "                x = int(hand_landmarks.landmark[8].x * screen_width)\n",
    "                y = int(hand_landmarks.landmark[8].y * screen_height)\n",
    "                \n",
    "                # Smooth cursor movement\n",
    "                x = int(prev_x * (1 - smoothing_factor) + x * smoothing_factor)\n",
    "                y = int(prev_y * (1 - smoothing_factor) + y * smoothing_factor)\n",
    "                prev_x, prev_y = x, y\n",
    "                \n",
    "                pyautogui.moveTo(x, y)\n",
    "                cv2.putText(img, \"Moving\", (10, 50), \n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Prediction error: {e}\")\n",
    "    \n",
    "    # Display the image\n",
    "    cv2.imshow(\"Virtual Mouse Gesture Control\", img)\n",
    "    \n",
    "    # Exit on 'q' key press\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57de6d94-200f-48a5-a203-b9734a9637f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
