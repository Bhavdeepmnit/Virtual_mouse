{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "103f2188-0808-4ef0-b025-d7cf279ab08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import os\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d977a5ae-1ea6-4c90-b6fa-df5927d4330f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)  # Use webcam\n",
    "\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(static_image_mode=False, max_num_hands=1, min_detection_confidence=0.7)\n",
    "mp_draw = mp.solutions.drawing_utils\n",
    "\n",
    "while True:\n",
    "    success, img = cap.read()\n",
    "    if not success:\n",
    "        continue\n",
    "    \n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    results = hands.process(img_rgb)\n",
    "    \n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            mp_draw.draw_landmarks(img, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "    \n",
    "    cv2.imshow(\"Hand Tracking\", img)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22e654ff-13d3-46c8-8fbb-99d4548572b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 100 samples for palm_open\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Initialize MediaPipe Hands\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(static_image_mode=False, max_num_hands=1, min_detection_confidence=0.7)\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Create dataset directory\n",
    "DATA_DIR = 'gesture_data'\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    os.makedirs(DATA_DIR)\n",
    "\n",
    "# Define your gestures\n",
    "gestures = ['index_up', 'fist', 'palm_open']\n",
    "samples_per_gesture = 100  # Number of samples to collect per gesture\n",
    "\n",
    "# Initialize webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "for gesture in gestures:\n",
    "    # Create subdirectory for each gesture\n",
    "    gesture_dir = os.path.join(DATA_DIR, gesture)\n",
    "    if not os.path.exists(gesture_dir):\n",
    "        os.makedirs(gesture_dir)\n",
    "    \n",
    "    print(f'Collecting data for {gesture}. Press \"q\" to skip this gesture.')\n",
    "    print('Get ready in 3 seconds...')\n",
    "    sleep(3)\n",
    "    \n",
    "    sample_count = 0\n",
    "    while sample_count < samples_per_gesture:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            continue\n",
    "            \n",
    "        # Flip frame horizontally for a mirror effect\n",
    "        frame = cv2.flip(frame, 1)\n",
    "        \n",
    "        # Convert to RGB and process with MediaPipe\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results = hands.process(frame_rgb)\n",
    "        \n",
    "        # Display countdown on screen\n",
    "        cv2.putText(frame, f'Collecting {gesture}: Sample {sample_count+1}/{samples_per_gesture}', \n",
    "                   (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "        \n",
    "        if results.multi_hand_landmarks:\n",
    "            hand_landmarks = results.multi_hand_landmarks[0]\n",
    "            \n",
    "            # Draw hand landmarks\n",
    "            mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "            \n",
    "            # Extract landmarks and save\n",
    "            landmarks = []\n",
    "            for landmark in hand_landmarks.landmark:\n",
    "                landmarks.extend([landmark.x, landmark.y, landmark.z])\n",
    "            \n",
    "            np.save(os.path.join(gesture_dir, f'{sample_count}.npy'), np.array(landmarks))\n",
    "            sample_count += 1\n",
    "            \n",
    "            # Small delay between samples\n",
    "            sleep(0.1)\n",
    "        \n",
    "        cv2.imshow('Data Collection', frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b929238-4f3b-4359-b84d-981e2f263966",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'gesture_data/index_up'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m X, y \u001b[38;5;241m=\u001b[39m [], []\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, gesture \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(gestures):\n\u001b[1;32m---> 10\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgesture_data/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mgesture\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m:\n\u001b[0;32m     11\u001b[0m         data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgesture_data/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgesture\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     12\u001b[0m         X\u001b[38;5;241m.\u001b[39mappend(data)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'gesture_data/index_up'"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "gestures = [\"index_up\", \"fist\", \"palm_open\"]\n",
    "X, y = [], []\n",
    "\n",
    "for idx, gesture in enumerate(gestures):\n",
    "    for file in os.listdir(f\"gesture_data/{gesture}\"):\n",
    "        data = np.load(f\"gesture_data/{gesture}/{file}\")\n",
    "        X.append(data)\n",
    "        y.append(idx)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "model = SVC(kernel='linear')\n",
    "model.fit(X_train, y_train)\n",
    "print(\"Accuracy:\", model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8fde8e-d7bd-4610-b38a-8046d47b1869",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "joblib.dump(model, \"gesture_model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f9623d-9261-4632-a16f-a16737b58148",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyautogui\n",
    "import joblib\n",
    "\n",
    "model = joblib.load(\"gesture_model.pkl\")\n",
    "screen_width, screen_height = pyautogui.size()\n",
    "\n",
    "while True:\n",
    "    success, img = cap.read()\n",
    "    if not success:\n",
    "        continue\n",
    "    \n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    results = hands.process(img_rgb)\n",
    "    \n",
    "    if results.multi_hand_landmarks:\n",
    "        hand_landmarks = results.multi_hand_landmarks[0]\n",
    "        data = []\n",
    "        for landmark in hand_landmarks.landmark:\n",
    "            data.extend([landmark.x, landmark.y, landmark.z])\n",
    "        \n",
    "        prediction = model.predict([data])[0]\n",
    "        \n",
    "        if prediction == 0:  # ðŸ‘† Index Up â†’ Left Click\n",
    "            pyautogui.click()\n",
    "        elif prediction == 1:  # âœŠ Fist â†’ Right Click\n",
    "            pyautogui.rightClick()\n",
    "        elif prediction == 2:  # âœ‹ Palm Open â†’ Move Cursor\n",
    "            x = int(hand_landmarks.landmark[8].x * screen_width)\n",
    "            y = int(hand_landmarks.landmark[8].y * screen_height)\n",
    "            pyautogui.moveTo(x, y)\n",
    "    \n",
    "    cv2.imshow(\"Virtual Mouse\", img)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57de6d94-200f-48a5-a203-b9734a9637f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
